{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'learning', 'data', 'science', ',', 'you', 'should', \"n't\", 'get', 'discouraged', '!', '\\n', 'Challenges', 'and', 'setbacks', 'are', \"n't\", 'failures', ',', 'they', \"'re\", 'just', 'part', 'of', 'the', 'journey', '.', 'You', \"'ve\", 'got', 'this', '!']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "text = \"\"\"When learning data science, you shouldn't get discouraged!\n",
    "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\"\n",
    "\n",
    "# For Words\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "my_doc = nlp(text)\n",
    "\n",
    "token_list = []\n",
    "for token in my_doc:\n",
    "    token_list.append(token.text)\n",
    "\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[When learning data science, you shouldn't get discouraged!, \n",
      "Challenges and setbacks aren't failures, they're just part of the journey., You've got this!]\n"
     ]
    }
   ],
   "source": [
    "# For Sentences\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "sbd = nlp.create_pipe('sentencizer')\n",
    "\n",
    "nlp.add_pipe(sbd)\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "sents_list = []\n",
    "for sent in doc.sents:\n",
    "    sents_list.append(sent)\n",
    "\n",
    "print(sents_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Sentence: [learning, data, science, ,, discouraged, !, \n",
      ", Challenges, setbacks, failures, ,, journey, ., got, !]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "filtered_sent = []\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for word in doc:\n",
    "    if word.is_stop == False:\n",
    "        filtered_sent.append(word)\n",
    "\n",
    "print(\"Filtered Sentence:\", filtered_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run run\n",
      "runs runs\n",
      "running running\n",
      "runner runner\n"
     ]
    }
   ],
   "source": [
    "lem = nlp(\"run runs running runner\")\n",
    "for word in lem:\n",
    "    print(word.text, word.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All DET\n",
      "is AUX\n",
      "well ADJ\n",
      "that DET\n",
      "ends VERB\n",
      "well ADV\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "docs = nlp(u\"All is well that ends well.\")\n",
    "for word in docs:\n",
    "    print(word.text, word.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(New York City, 'GPE', 384), (Tuesday, 'DATE', 391), (At least 285, 'CARDINAL', 397), (September, 'DATE', 391), (Brooklyn, 'GPE', 384), (Williamsburg, 'GPE', 384), (four, 'CARDINAL', 397), (Bill de Blasio, 'PERSON', 380), (Tuesday, 'DATE', 391), (Orthodox Jews, 'PERSON', 380), (6 months old, 'DATE', 391), (up to $1,000, 'MONEY', 394)]\n"
     ]
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "nytimes = nlp(u\"\"\"\n",
    "New York City on Tuesday declared a public health emergency and ordered mandatory measles vaccinations amid an outbreak, becoming the latest national flash point over refusals to inoculate against dangerous diseases.\n",
    "\n",
    "At least 285 people have contracted measles in the city since September, mostly in Brooklyn’s Williamsburg neighborhood. The order covers four Zip codes there, Mayor Bill de Blasio (D) said Tuesday.\n",
    "\n",
    "The mandate orders all unvaccinated people in the area, including a concentration of Orthodox Jews, to receive inoculations, including for children as young as 6 months old. Anyone who resists could be fined up to $1,000.\"\"\")\n",
    "\n",
    "entities = [(i, i.label_, i.label) for i in nytimes.ents]\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"></br>\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    New York City\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " on \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tuesday\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " declared a public health emergency and ordered mandatory measles vaccinations amid an outbreak, becoming the latest national flash point over refusals to inoculate against dangerous diseases.</br></br>\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    At least 285\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " people have contracted measles in the city since \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    September\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", mostly in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Brooklyn\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "’s \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Williamsburg\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " neighborhood. The order covers \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    four\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " Zip codes there, Mayor \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bill de Blasio\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " (D) said \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tuesday\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</br></br>The mandate orders all unvaccinated people in the area, including a concentration of \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Orthodox Jews\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", to receive inoculations, including for children as young as \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    6 months old\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ". Anyone who resists could be fined \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    up to $1,000\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(nytimes, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pursuit pursuit pobj In\n",
      "a wall wall pobj of\n",
      "President Trump Trump nsubj ran\n"
     ]
    }
   ],
   "source": [
    "docp = nlp(\" In pursuit of a wall, President Trump ran into one.\")\n",
    "\n",
    "for chunk in docp.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"> In pursuit of a wall, President \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Trump\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " ran into one.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(docp, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vector Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96,)\n",
      "[ 0.20538223 -1.6033714   0.27122334  0.4102599   3.2985601   3.4889512\n",
      "  1.8090308  -2.1398475   2.31565     1.5809067   4.1519527  -1.0185633\n",
      " -0.0325011  -2.7471437  -0.4177467  -2.4292274  -0.6153387   2.4422317\n",
      "  0.8078671  -2.4846377   2.0988142   1.4448209  -0.552992   -1.3411183\n",
      " -0.69847786 -0.45548356  3.8267968  -4.0225782   0.81215733  0.3766132\n",
      "  0.15751392 -1.1428392  -1.3328214   0.7187766   2.1567593  -3.018766\n",
      "  3.4919028   0.6938907  -1.1943094  -0.10796624  4.7029977   3.551554\n",
      " -0.71505725 -4.4580555  -0.26480573  0.6314918  -0.538128   -1.1131921\n",
      " -1.1251849   0.5740081  -1.1976193  -3.5157654   0.425157   -1.7545594\n",
      " -3.058784    0.01680815  0.97784567  1.7633746   0.4561966   2.5090182\n",
      "  0.35267782  0.8351371  -1.394351    0.5082075   0.75960976 -3.3654122\n",
      "  2.3440146  -2.4311178   1.2401564  -1.4498216  -2.3708577   1.274456\n",
      "  2.6584334   2.505236    0.24999112  0.45838034  0.7396465  -3.0134087\n",
      " -1.1449497   2.441533    0.58746856 -0.47240722 -0.99527466 -2.6816308\n",
      "  0.09063269  6.415723   -2.0405467   0.45456767 -1.6633052  -0.92034495\n",
      "  0.12515879 -1.3880293  -2.1336856   2.3337066  -0.68828297 -1.3300103 ]\n"
     ]
    }
   ],
   "source": [
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "mango = nlp(u\"mango\")\n",
    "\n",
    "print(mango.vector.shape)\n",
    "print(mango.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>variation</th>\n",
       "      <th>verified_reviews</th>\n",
       "      <th>feedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Love my Echo!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Loved it!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Walnut Finish</td>\n",
       "      <td>Sometimes while playing a game, you can answer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>I have had a lot of fun with this thing. My 4 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>31-Jul-18</td>\n",
       "      <td>Charcoal Fabric</td>\n",
       "      <td>Music</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating       date         variation  \\\n",
       "0       5  31-Jul-18  Charcoal Fabric    \n",
       "1       5  31-Jul-18  Charcoal Fabric    \n",
       "2       4  31-Jul-18    Walnut Finish    \n",
       "3       5  31-Jul-18  Charcoal Fabric    \n",
       "4       5  31-Jul-18  Charcoal Fabric    \n",
       "\n",
       "                                    verified_reviews  feedback  \n",
       "0                                      Love my Echo!         1  \n",
       "1                                          Loved it!         1  \n",
       "2  Sometimes while playing a game, you can answer...         1  \n",
       "3  I have had a lot of fun with this thing. My 4 ...         1  \n",
       "4                                              Music         1  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_amazon = pd.read_csv(\"datasets/amazon_alexa.tsv\", sep=\"\\t\")\n",
    "df_amazon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3150, 5)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_amazon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of       rating       date         variation  \\\n",
       "0          5  31-Jul-18  Charcoal Fabric    \n",
       "1          5  31-Jul-18  Charcoal Fabric    \n",
       "2          4  31-Jul-18    Walnut Finish    \n",
       "3          5  31-Jul-18  Charcoal Fabric    \n",
       "4          5  31-Jul-18  Charcoal Fabric    \n",
       "...      ...        ...               ...   \n",
       "3145       5  30-Jul-18        Black  Dot   \n",
       "3146       5  30-Jul-18        Black  Dot   \n",
       "3147       5  30-Jul-18        Black  Dot   \n",
       "3148       5  30-Jul-18        White  Dot   \n",
       "3149       4  29-Jul-18        Black  Dot   \n",
       "\n",
       "                                       verified_reviews  feedback  \n",
       "0                                         Love my Echo!         1  \n",
       "1                                             Loved it!         1  \n",
       "2     Sometimes while playing a game, you can answer...         1  \n",
       "3     I have had a lot of fun with this thing. My 4 ...         1  \n",
       "4                                                 Music         1  \n",
       "...                                                 ...       ...  \n",
       "3145  Perfect for kids, adults and everyone in betwe...         1  \n",
       "3146  Listening to music, searching locations, check...         1  \n",
       "3147  I do love these things, i have them running my...         1  \n",
       "3148  Only complaint I have is that the sound qualit...         1  \n",
       "3149                                               Good         1  \n",
       "\n",
       "[3150 rows x 5 columns]>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_amazon.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2893\n",
       "0     257\n",
       "Name: feedback, dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_amazon.feedback.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "punctuations = string.punctuation\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "parser = English()\n",
    "\n",
    "def spacy_tokenizer(sentence):\n",
    "    my_tokens = parser(sentence)\n",
    "    my_tokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in my_tokens]\n",
    "    my_tokens = [word for word in my_tokens if word not in STOP_WORDS and word not in punctuations]\n",
    "    return my_tokens\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.strip().lower()\n",
    "\n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        return [clean_text(text) for text in X]\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = CountVectorizer(tokenizer=spacy_tokenizer, ngram_range=(1,1))\n",
    "tfidf_vector = TfidfVectorizer(tokenizer=spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_amazon['verified_reviews']\n",
    "y_labels = df_amazon['feedback']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/foxgem/miniconda3/envs/tfenv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cleaner', <__main__.predictors object at 0x1181f3dd8>),\n",
       "                ('vectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 toke...?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function spacy_tokenizer at 0x1187f8f28>,\n",
       "                                 vocabulary=None)),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='warn', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "pipeline = Pipeline([(\"cleaner\", predictors()),\n",
    "                    (\"vectorizer\", bow_vector),\n",
    "                    (\"classifier\", classifier)])\n",
    "\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9333333333333333\n",
      "Logistic Regression Precision: 0.937636761487965\n",
      "Logistic Regression Recall: 0.9930475086906141\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "predicted = pipeline.predict(X_test)\n",
    "\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
